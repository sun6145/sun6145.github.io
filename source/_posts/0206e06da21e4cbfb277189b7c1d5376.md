---
layout: post
title: Kubernetes快速安装
abbrlink: 0206e06da21e4cbfb277189b7c1d5376
tags:
  - kubernetes
categories:
  - 云原生
  - K8S
date: 1746403380000
updated: 1746529419114
---

Kubernetes（K8s）是当前最流行的容器编排工具，帮助开发者高效管理容器化应用。本文将介绍 K8s 的核心概念，包括 Pod、Deployment 和 Service，以及如何快速搭建一个简单的集群。无论你是初学者还是希望巩固基础，这篇指南都能助你轻松迈入云原生世界。

<!-- more -->

***

## 背景

### 部署方式的变迁

![部署演进](/resources/fcac9c96d9fe45818940f81724caa2be.svg)

### 传统物理机部署

- **特点**：
  - 直接运行在物理服务器上，每个应用独占硬件资源。
  - 依赖操作系统和底层硬件，资源分配静态且固定。
- 问题：
  - **资源利用率低**：应用无法共享资源，容易造成浪费。
  - **扩展性差**：横向扩展需要新增物理机，成本高、周期长。
  - **环境依赖强**：应用与硬件/OS 强绑定，迁移和运维复杂。

### 虚拟化部署

- **特点**：
  - 通过 Hypervisor（如 VMware、KVM）在物理机上虚拟出多个虚拟机（VM），每个 VM 运行独立 OS 和应用。
  - 资源动态分配，支持多租户隔离。
- 优势：
  - 提高了硬件利用率，降低了成本。
  - 环境隔离性好，安全性较高。
- 局限性：
  - **资源开销大**：每个 VM 需运行完整 OS，占用额外 CPU、内存。
  - **启动慢**：VM 启动需要分钟级时间。
  - **管理复杂**：大规模 VM 集群的编排、调度仍需手动或依赖工具（如 OpenStack）。

### 容器化部署

- 特点：
  - 基于容器技术（如 Docker），共享主机 OS 内核，轻量级进程隔离。
  - 应用打包为镜像，实现跨环境一致性（开发、测试、生产）。
- **容器优势：**
  - \*\*敏捷性：\*\*敏捷应用程序的创建和部署：与使用 VM 镜像相比，提高了容器镜像创建的简便性和效率。
  - \*\*及时性：\*\*持续开发、集成和部署：通过快速简单的回滚（由于镜像不可变性），支持可靠且频繁的 容器镜像构建和部署。
  - \*\*解耦性：\*\*关注开发与运维的分离：在构建/发布时创建应用程序容器镜像，而不是在部署时。 从而将应用程序与基础架构分离。
  - \*\*可观测性：\*\*可观察性不仅可以显示操作系统级别的信息和指标，还可以显示应用程序的运行状况和其他指标信号。
  - \*\*跨平台：\*\*跨开发、测试和生产的环境一致性：在便携式计算机上与在云中相同地运行。
  - \*\*可移植：\*\*跨云和操作系统发行版本的可移植性：可在 Ubuntu、RHEL、CoreOS、本地、 Google Kubernetes Engine 和其他任何地方运行。
  - \*\*简易性：\*\*以应用程序为中心的管理：提高抽象级别，从在虚拟硬件上运行 OS 到使用逻辑资源在 OS 上运行应用程序。
  - \*\*大分布式：\*\*松散耦合、分布式、弹性、解放的微服务：应用程序被分解成较小的独立部分， 并且可以动态部署和管理 - 而不是在一台大型单机上整体运行。
  - \*\*隔离性：\*\*资源隔离：可预测的应用程序性能。
  - **高效性：**资源利用：高效率和**高密度**
- 新挑战：
  - **编排复杂性**：大规模容器集群的调度、网络、存储、扩缩容等问题。
  - **高可用性**：如何自动恢复故障容器？
  - **服务发现**：动态变化的容器如何通信？

### Kubernetes 的诞生背景

1. **容器编排的需求**：
   Docker 等容器技术解决了应用打包问题，但缺乏管理大规模容器的能力。早期工具（如 Docker Swarm、Mesos）功能有限。
2. **Google 的 Borg 系统经验**：
   Kubernetes 源自 Google 内部的 ​**​Borg​**​ 系统（管理千万级容器的平台），吸收了其多年的大规模集群管理经验。
3. **开源生态的推动**：
   - 2014 年 Google 开源 K8s，并捐赠给 CNCF（云原生计算基金会），推动标准化。
   - 解决了多云/混合云场景下的应用部署一致性。
4. **核心价值**：
   - **自动化运维**：自动调度、扩缩容、滚动更新、自愈（如重启故障容器）。
   - **声明式配置**：通过 YAML 文件定义应用状态，而非手动操作。
   - **扩展性**：支持插件化架构（如 CNI 网络插件、CSI 存储插件）。

### 为什么用 Kubernetes

Kubernetes（K8s）之所以被广泛采用，是因为它解决了现代应用部署和运维中的核心痛点，尤其是在**云原生**和**微服务架构**的背景下。

|   **需求**  |           **K8s 的解决方案**          |
| :-------: | :------------------------------: |
|   自动化运维   |           自愈、滚动更新、自动扩缩容          |
|   资源利用率高  |           容器轻量化 + 动态调度           |
|   跨环境部署   |          多云/混合云支持，避免依赖差异         |
|   微服务管理   | Service 发现、负载均衡、ConfigMap/Secret |
|    可扩展性   |        CNI/CSI/CRD，支持自定义扩展       |
| 生态 & 社区支持 |    Helm、Istio、Prometheus 等丰富工具   |

## Kubernetes 简介

> 官方文档 ：[Kubernetes 文档 | Kubernetes](https://kubernetes.io/zh-cn/docs/home/)

Kubernetes（简称 K8s）是一个开源的容器编排平台，用于自动化部署、扩展和管理容器化应用。它源自 Google 内部的大规模生产经验（[Google 在大规模运行生产工作负载方面拥有十几年的经验](https://research.google/pubs/pub43438)），于 2014 年开源，现由云原生计算基金会（CNCF）维护。名称 **Kubernetes** 源于希腊语，意为“舵手”或“飞行员”

### 核心特性

1. 自动化运维
   • 自愈：自动重启故障容器，替换异常节点上的 Pod。

   • 滚动更新与回滚：分阶段更新应用，失败时自动回滚。

   • 动态扩缩容：根据负载自动调整 Pod 数量（HPA）。

2. 跨环境一致性
   • 支持多云（公有云、私有云、混合云），通过容器镜像实现开发-生产环境一致。

3. 声明式配置
   • 通过 YAML 定义应用状态（如 Deployment、Service），系统自动协调实现。

4. 服务发现与负载均衡
   • 内置 DNS 和 Service 机制，自动管理微服务通信与流量分发。

5. 存储与配置管理
   • 支持动态挂载存储（如 NFS、云存储），通过 ConfigMap/Secret 管理配置和敏感数据。

### Kubernetes 不是什么

- 非 PaaS：不提供中间件、数据库等应用级服务，仅提供底层编排能力。
- 非 CI/CD 工具：不处理源码构建，需结合外部工具（如 Jenkins）。
- 非硬性监控方案：指标收集和告警需集成第三方工具（如 Prometheus）。

### 适用场景

- 微服务架构、大规模容器化应用、需高可用和弹性伸缩的场景。

- 不适合小型单体应用或资源有限的团队（运维复杂度高）。

## 架构核心

Kubernetes（K8s）采用 **Master-Node 架构**，将集群分为\*\*控制平面（Master）**和**工作节点（Node）\*\*两部分，分别负责集群管理和实际应用运行。

- Master 节点：包含 API Server（集群入口）、Scheduler（资源调度）、Controller Manager（状态维护）、etcd（集群存储）。
- Worker 节点：运行 kubelet（容器生命周期管理）、kube-proxy（网络代理）、容器运行时（如 Docker）。

![components-of-kubernetes.svg](/resources/647c48a702004f4aa7a50f309773fd68.svg)

### Master 节点（Control Plane）

负责集群的全局管理和调度，核心组件如下：

| 组件                 | 职责                                                                    |
| ------------------ | --------------------------------------------------------------------- |
| API Server         | 集群唯一入口（类似 MVC 中的 Controller），接收所有请求（如 `kubectl` 命令），验证并持久化到 etcd。     |
| etcd               | 分布式键值数据库，存储集群所有状态（如节点信息、Pod 配置）。相当于集群的“记账本”。                          |
| Scheduler          | 调度器，根据资源需求（CPU/内存）、亲和性等规则，决定 Pod 运行在哪个 Node 上。                        |
| Controller Manager | 运行多种控制器（如 Deployment Controller、Node Controller），确保集群实际状态与期望一致（如副本数）。 |

### Node 节点（工作节点）

负责运行容器化应用，核心组件如下：

| 组件         | 职责                                                  |
| ---------- | --------------------------------------------------- |
| kubelet    | 节点“监工”，与 Master 通信，管理本节点 Pod 生命周期（如启动/停止容器），并上报状态。  |
| kube-proxy | 网络代理，维护 Service 的流量规则（如 iptables/IPVS），实现负载均衡和服务发现。 |
| 容器运行时      | 实际运行容器的引擎（如 Docker、containerd），负责拉取镜像、启停容器。         |

### 架构对比总结

| 组件/角色 | Master 节点         | Node 节点             |
| ----- | ----------------- | ------------------- |
| 核心职责  | 全局调度、状态管理         | 运行容器、网络代理           |
| 数据存储  | etcd（集群状态）        | 无（仅上报状态至 Master）    |
| 通信方式  | API Server 接收所有请求 | kubelet 主动拉取任务并上报状态 |
| 自动化能力 | 自愈、扩缩容、滚动更新       | 容器生命周期管理、流量转发       |

## 应用部署流程详解

• Master 节点是集群的“大脑”，通过 `API Server`、`Scheduler`、`Controller Manager` 实现全局控制。

• Node 节点是“执行者”，依赖 `kubelet` 和 `kube-proxy` 运行容器并管理网络。

• 部署流程本质是“声明式驱动”：用户提交期望状态 → Master 调度 → Node 执行 → 状态同步闭环。

以下以部署一个 `tomcat` 应用为例，说明 Kubernetes 的工作流程：

### 阶段 1：用户提交请求

- 用户操作：通过 CLI（如 `kubectl`）提交部署请求：

  ```sh
  kubectl create deployment tomcat --image=tomcat:6 --port=8080
  ```

- 流程：

  - 请求发送至 Master 的 API Server（集群唯一入口）。
  - API Server 验证请求合法性后，将部署信息（如镜像、端口）记录到 etcd。

### 阶段 2：调度与任务分配

- Scheduler 调度：

  1. Scheduler 监听 etcd，发现未调度的 Pod。

  2. 根据资源需求、节点负载等策略，选择最优 Node（如 Node2）。

  3. 将调度结果（Pod → Node2）写入 etcd。

- Controller Manager 监控：
  - Deployment Controller 持续检查 Pod 副本数是否符合期望，若不一致（如 Pod 崩溃），触发自愈操作（如重建 Pod）。

### 阶段 3：节点执行部署

- kubelet 执行：

  1. Node2 的 kubelet 通过监听 API Server，获取分配给本节点的 Pod 任务。

  2. 调用容器运行时启动 `tomcat` 容器，分配 IP 和存储资源。

  3. 持续监控容器状态，并上报至 API Server（最终存储到 etcd）。

### 阶段 4：网络与服务发现

- kube-proxy 配置网络：
  - kube-proxy 监听 Service 和 Endpoints 变化。
  - 自动更新 iptables/IPVS 规则，将访问 `tomcat` Service 的流量转发到后端 Pod（如 Node2 的容器）。

## Kubernetes 安装

### 安装方式

- **二进制方式安装**

  - **手动部署每个组件**（如 `etcd`、`kube-apiserver`、`kube-controller-manager`、`kube-scheduler`、`kubelet`、`kube-proxy`）。

  - **完全控制组件的配置和版本**，适合深度定制化需求。

- kubeadm 引导方式（官方推荐）

  - **官方工具自动化部署**，核心组件（如 `apiserver`）以容器形式运行。
  - `kubeadm` 支持无缝升级，适合追求稳定更新的场景。

-

### 大致流程

- 准备 N 台服务器，**内网互通**，

- 安装 Docker 容器化环境

  - 自 Kubernetes v1.24 起，**弃用 Docker**（仍可通过 `cri-dockerd` 兼容）。
  - **推荐使用 `containerd` 或 `CRI-O`**，直接集成 CRI 接口。

- 安装 Kubernetes
  - 三台机器安装核心组件（**kubeadm(创建集群的引导工具)**, ***kubelet***，**kubectl（程序员用的命令行）** ）
  - kubelet 可以直接通过容器化的方式创建出之前的核心组件（api-server）【官方把核心组件做成镜像】
  - 由 kubeadm 引导创建集群

## kubeadm 创建集群

[使用 kubeadm 引导集群 | Kubernetes](https://kubernetes.io/zh-cn/docs/setup/production-environment/tools/kubeadm/)

### 准备机器

- 开通三台 2c4g 机器，内网互通，配置公网 ip。二台也可以

  - 每台机器 2 GB 或更多的 RAM（如果少于这个数字将会影响你应用的运行内存）。
  - master 机器需要 CPU 2 核心或更多。

- 每台机器的 hostname 不要用 localhost，可用 k8s-01，k8s-02，k8s-03 之类的【不包含下划线、小数点、大写字母】

### 安装前置环境（都执行）

#### 基础环境

- 确保每个节点上 MAC 地址和 product\_uuid 的唯一性

  ```sh
  # 一般来讲，硬件设备会拥有唯一的地址，但是有些虚拟机的地址可能会重复。
  # 可以使用命令 ip link 或 ifconfig -a 来获取网络接口的 MAC 地址
  # 可以使用 sudo cat /sys/class/dmi/id/product_uuid 命令对 product_uuid 校验
  ```

- **时间同步**：所有节点需启用 NTP（如 `chronyd` 或 `ntpd`），避免时间偏差导致证书错误。

- 关闭防火墙： 如果是云服务器，需要设置安全组策略放行端口

  ```sh
  # https://kubernetes.io/zh/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#check-required-ports
  systemctl stop firewalld
  systemctl disable firewalld
  ```

- 修改 hostname

  ```sh
  hostnamectl set-hostname k8s-01
  # 查看修改结果
  hostnamectl status
  # 设置 hostname 解析
  echo "127.0.0.1   $(hostname)" >> /etc/hosts
  ```

- 关闭 selinux：

  ```sh
  sudo setenforce 0
  # 将 SELinux 设置为 permissive 模式（相当于将其禁用）
  sudo sed -i 's/^SELINUX=enforcing$/SELINUX=permissive/' /etc/selinux/config
  # 直接关闭
  sudo sed -i 's/enforcing/disabled/' /etc/selinux/config
  ```

- 关闭 swap：

  ```sh
  #kubelet 的默认行为是在节点上检测到交换内存时无法启动。 这意味着要么禁用交换（swap）功能，要么让 kubelet 容忍交换。
  swapoff -a
  sed -ri 's/.*swap.*/#&/' /etc/fstab
  ```

- 加载 br\_netfilter 内核模块

  ```sh
  # 临时加载模块（立即生效）
  sudo modprobe br_netfilter
  ## 确认下
  lsmod | grep br_netfilter  # 应看到输出
  # 永久加载（重启后依然有效）
  echo "br_netfilter" | sudo tee /etc/modules-load.d/k8s.conf
  ```

- 启用桥接流量转发到 iptables

  ```sh
  # 永久生效（重启后仍有效）：
  cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
  # 启用 IPv4 转发（必需，否则 Pod 跨节点通信可能失败）
  net.ipv4.ip_forward = 1

  # 允许桥接流量经过 iptables（必需，否则 Calico/Flannel 等 CNI 插件无法正常工作）
  net.bridge.bridge-nf-call-iptables = 1
  net.bridge.bridge-nf-call-ip6tables = 1

  # 禁用 IPv6（可选，避免 Kubernetes 因 IPv6 配置问题报错）
  net.ipv6.conf.all.disable_ipv6 = 1
  net.ipv6.conf.default.disable_ipv6 = 1
  net.ipv6.conf.lo.disable_ipv6 = 1

  # 如果仍需 IPv6，启用转发（可选，通常不需要）
  # net.ipv6.conf.all.forwarding = 1
  EOF

  # 重新加载配置
  sudo sysctl --system
  ```

#### docker 环境

```sh
sudo yum remove docker*
sudo yum install -y yum-utils
#配置docker yum 源
sudo yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo

#安装docker 19.03.9   docker-ce  19.03.9
# yum install -y docker-ce-3:19.03.9-3.el7.x86_64  docker-ce-cli-3:19.03.9-3.el7.x86_64 containerd.io
yum install -y docker-ce-19.03.9-3  docker-ce-cli-19.03.9 containerd.io

#启动服务
systemctl start docker
systemctl enable docker

#配置加速
sudo mkdir -p /etc/docker
sudo tee /etc/docker/daemon.json <<-'EOF'
{
  "registry-mirrors": ["https://82m9ar63.mirror.aliyuncs.com"]
}
EOF
sudo systemctl daemon-reload
sudo systemctl restart docker
```

### 安装 k8s 核心（都执行）

```sh
# 配置K8S的yum源
cat <<EOF > /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=0
repo_gpgcheck=0
gpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg
       http://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
EOF

# 卸载旧版本
yum remove -y kubelet kubeadm kubectl

# 查看可以安装的版本
yum list kubelet --showduplicates | sort -r

# 安装kubelet、kubeadm、kubectl 指定版本
yum install -y kubelet-1.21.0 kubeadm-1.21.0 kubectl-1.21.0

# 开机启动kubelet
systemctl enable kubelet && systemctl start kubelet
```

### 初始化 master 节点

- master 执行,下载核心镜像

  ```sh
  # 查看需要哪些镜像
  $ kubeadm config images list
  k8s.gcr.io/kube-apiserver:v1.21.0
  k8s.gcr.io/kube-controller-manager:v1.21.0
  k8s.gcr.io/kube-scheduler:v1.21.0
  k8s.gcr.io/kube-proxy:v1.21.0
  k8s.gcr.io/pause:3.4.1
  k8s.gcr.io/etcd:3.4.13-0
  k8s.gcr.io/coredns/coredns:v1.8.0
  ```

- 拉取镜像，封装 images.sh 文件

  ```sh
  cat <<EOF > ~/images.sh
  #!/bin/bash
  images=(
    kube-apiserver:v1.21.0
    kube-proxy:v1.21.0
    kube-controller-manager:v1.21.0
    kube-scheduler:v1.21.0
    coredns:v1.8.0
    etcd:3.4.13-0
    pause:3.4.1
  )
  for imageName in ${images[@]} ; do
      docker pull registry.cn-hangzhou.aliyuncs.com/lfy_k8s_images/$imageName
  done
  EOF

  chmod +x ~/images.sh && ~/images.sh
  ```

- 修改镜像标签

  ```sh
  # registry.cn-hangzhou.aliyuncs.com/lfy_k8s_images/coredns:v1.8.0  k8s.gcr.io/coredns/coredns:v1.8.0

  ##注意1.21.0版本的k8s coredns镜像比较特殊，结合阿里云需要特殊处理，重新打标签
  docker tag registry.cn-hangzhou.aliyuncs.com/lfy_k8s_images/coredns:v1.8.0 registry.cn-hangzhou.aliyuncs.com/lfy_k8s_images/coredns/coredns:v1.8.0
  ```

- kubeadm 初始化 master

  - `--apiserver-advertise-address` 的配置是**主节点的正确 IP**
  - 网络 无类别域间路由（Classless Inter-Domain Routing、CIDR） 配置
    - Pod 网络插件兼容性：`--pod-network-cidr=192.170.0.0/16` 需与后续安装的网络插件匹配。Calico：默认兼容此 CIDR。
    - 避免 CIDR 冲突：确保 `10.96.0.0/16（Service CIDR）`和 `192.170.0.0/16（Pod CIDR）`不与主机网络或现有 VPN 重叠。

  ```sh
  # 注意：pod的子网范围+service负载均衡网络的子网范围+本机ip的子网范围不能有重复域
  kubeadm init \
  --apiserver-advertise-address=192.168.61.11 \
  --image-repository registry.cn-hangzhou.aliyuncs.com/lfy_k8s_images \
  --kubernetes-version v1.21.0 \
  --service-cidr=10.96.0.0/16 \
  --pod-network-cidr=192.170.0.0/16
  ```

- **重新初始化集群**

  ```sh
  # 重置 kubeadm 安装状态（删除所有 Kubernetes 组件）
  sudo kubeadm reset -f

  # 清理残留配置
  sudo rm -rf /etc/kubernetes/ /var/lib/kubelet/ /var/lib/etcd/ /etc/cni/net.d/

  # 删除 kubectl 配置文件（如有）
  rm -rf $HOME/.kube/
  ```

- 按照提示继续

  ```sh
  Your Kubernetes control-plane has initialized successfully!

  To start using your cluster, you need to run the following as a regular user:
    ## init完成后第一步：复制相关文件夹
    mkdir -p $HOME/.kube
    sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
    sudo chown $(id -u):$(id -g) $HOME/.kube/config

  Alternatively, if you are the root user, you can run:
    ## 导出环境变量
    export KUBECONFIG=/etc/kubernetes/admin.conf
  ```

- 部署一个 pod 网络
  [calico.yaml 文件备份](/resources/e632927f1d94433e938023de9ab1ac2b.yaml)

  ```sh
  ### 部署一个pod网络
  You should now deploy a pod network to the cluster.
  Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
    https://kubernetes.io/docs/concepts/cluster-administration/addons/

  ### 手动拉取所需镜像
  docker pull swr.cn-north-4.myhuaweicloud.com/ddn-k8s/docker.io/calico/kube-controllers:v3.25.0
  docker tag  swr.cn-north-4.myhuaweicloud.com/ddn-k8s/docker.io/calico/kube-controllers:v3.25.0  docker.io/calico/kube-controllers:v3.25.0

  ### 部署一个pod网络
  kubectl apply -f https://docs.projectcalico.org/manifests/calico.yaml
  ```

- 命令检查

  ```sh
  ##获取集群中所有部署好的应用Pod
  ❯ kubectl get pod -A
  NAMESPACE     NAME                                       READY   STATUS    RESTARTS   AGE
  kube-system   calico-kube-controllers-68d86f8988-9f558   1/1     Running   0          38s
  kube-system   calico-node-4fs25                          1/1     Running   0          38s
  kube-system   coredns-b98666c6d-bxv75                    1/1     Running   0          44s
  kube-system   coredns-b98666c6d-t7pdt                    1/1     Running   0          44s
  kube-system   etcd-node01                                1/1     Running   0          61s
  kube-system   kube-apiserver-node01                      1/1     Running   0          61s
  kube-system   kube-controller-manager-node01             1/1     Running   0          61s
  kube-system   kube-proxy-q75nd                           1/1     Running   0          44s
  kube-system   kube-scheduler-node01                      1/1     Running   0          61s


  ##查看集群所有机器的状态
  ❯ kubectl get nodes
  NAME     STATUS   ROLES                  AGE    VERSION
  node01   Ready    control-plane,master   9m6s   v1.21.0

  ```

- 如何报错检查日志

  ```sh
  kubectl describe -n kube-system pod  calico-kube-controllers-68d86f8988-9f558

  Events:
    Type     Reason            Age                   From               Message
    ----     ------            ----                  ----               -------
    Warning  FailedScheduling  113s (x4 over 2m10s)  default-scheduler  0/1 nodes are available: 1 node(s) had taint {node.kubernetes.io/not-ready: }, that the pod didn't tolerate.
    Normal   Scheduled         105s                  default-scheduler  Successfully assigned kube-system/calico-kube-controllers-68d86f8988-9f558 to node01
    Normal   Pulled            103s                  kubelet            Container image "docker.io/calico/kube-controllers:v3.25.0" already present on machine
    Normal   Created           103s                  kubelet            Created container calico-kube-controllers
    Normal   Started           103s                  kubelet            Started container calico-kube-controller
  ```

### 初始化 worker 节点

- worker 节点执行

  ```sh
  ## 用master生成的命令即可
  Then you can join any number of worker nodes by running the following on each as root:

  kubeadm join 192.168.61.11:6443 --token hbo6ks.cv9bcmgln6henlaa \
  	--discovery-token-ca-cert-hash sha256:f89a34d215d256d215726724dea060742f327bcd270997c457880fab03c54e62



  ##过期怎么办
  kubeadm token create --print-join-command
  kubeadm token create --ttl 0 --print-join-command
  kubeadm join --token y1eyw5.ylg568kvohfdsfco --discovery-token-ca-cert-hash sha256: 6c35e4f73f72afd89bf1c8c303ee55677d2cdb1342d67bb23c852aba2efc7c73
  ```

- 重新加入节点

  ```sh
  # 1. 重置 kubeadm 状态
  sudo kubeadm reset -f

  # 2. 清理残留文件
  sudo rm -rf /etc/kubernetes/ /var/lib/kubelet/ /var/lib/etcd/ /etc/cni/net.d/

  # 3. 清理 iptables 规则（避免网络冲突）
  sudo iptables -F && sudo iptables -t nat -F && sudo iptables -t mangle -F

  # 4. 杀死可能占用 kubelet 端口的进程（如残留的 kubelet）
  sudo lsof -i :10250 | awk 'NR!=1 {print $2}' | xargs sudo kill -9 2>/dev/null
  ```

### 验证集群

```sh
#获取所有节点
❯ kubectl get nodes
NAME     STATUS     ROLES                  AGE    VERSION
node01   Ready      control-plane,master   3h9m   v1.21.0
node02   NotReady   <none>                 15s    v1.21.0

#给节点打标签
## k8s中万物皆对象。node:机器  Pod：应用容器
# 加标签  《h1》
kubectl label node node02 node-role.kubernetes.io/worker=''
# 去标签
kubectl label node node02 node-role.kubernetes.io/worker-


## k8s集群，机器重启了会自动再加入集群，master重启了会自动再加入集群控制中心
```

### 设置 ipvs 模式

k8s 整个集群为了访问通；默认是用 iptables,性能下（kube-proxy 在集群之间同步 iptables 的内容）

```sh
kubectl get pod -A
kube-system   kube-proxy-fvwsr                           1/1     Running   0          3h42m
kube-system   kube-proxy-q75nd                           1/1     Running   0          6h52m

#1、查看默认kube-proxy 使用的模式
kubectl logs -n kube-system kube-proxy-fvwsr
I0506 06:30:47.251356       1 server_others.go:206] kube-proxy running in dual-stack mode, IPv4-primary
I0506 06:30:47.251423       1 server_others.go:212] Using iptables Proxier.
#2、需要修改 kube-proxy 的配置文件,修改mode 为ipvs。默认iptables，但是集群大了以后就很慢
kubectl edit cm kube-proxy -n kube-system
# 修改 mode: "ipvs"
   ipvs:
      excludeCIDRs: null
      minSyncPeriod: 0s
      scheduler: ""
      strictARP: false
      syncPeriod: 30s
    kind: KubeProxyConfiguration
    metricsBindAddress: 127.0.0.1:10249
    mode: "ipvs"

### 修改了kube-proxy的配置，为了让重新生效，需要杀掉以前的Kube-proxy
kubectl get pod -A|grep kube-proxy
kubectl delete pod kube-proxy-fvwsr -n kube-system
kubectl delete pod kube-proxy-q75nd -n kube-system
### 修改完成后可以发现自动重启kube-proxy，配置会重新生效
❯ kubectl get pod -A|grep kube-proxy
kube-system   kube-proxy-kbtdn                           1/1     Running   0          20s
kube-system   kube-proxy-xqvxl                           1/1     Running   0          8s
```
