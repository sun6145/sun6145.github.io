---
layout: post
title: NVIDIAä¸‹ä½¿ç”¨ollama
abbrlink: 89a0150d9f8342ba92c12dfbd84b07f5
tags:
  - ollama
categories:
  - AI
  - OLLAMA
date: 1745339269294
updated: 1746416613231
---

æ‰‹æŠŠæ‰‹æ•™ä½ å¦‚ä½•åœ¨ä¸ªäººç”µè„‘ä¸Šéƒ¨ç½² OLLAMA å¤§æ¨¡å‹æ¡†æ¶ï¼Œä»ç¯å¢ƒé…ç½®åˆ°æ¨¡å‹åŠ è½½ï¼Œè¯¦ç»†è§£ææœ¬åœ°è¿è¡ŒæŠ€å·§ã€‚æ— éœ€äº‘ç«¯èµ„æºï¼Œè½»æ¾å®ç° AI æ¨¡å‹çš„ç¦»çº¿æ¨ç†ä¸å¾®è°ƒï¼Œé‡Šæ”¾æœ¬åœ°ç®—åŠ›æ½œèƒ½ã€‚

<!-- more -->

***

## å®‰è£… GPU é©±åŠ¨ç¨‹åº

> **è¿™æ˜¯æ‚¨å”¯ä¸€éœ€è¦å®‰è£…çš„é©±åŠ¨ç¨‹åºã€‚ä¸è¦åœ¨ WSL ä¸­å®‰è£…ä»»ä½• Linux æ˜¾ç¤ºé©±åŠ¨ç¨‹åºã€‚**

ä¸‹è½½å¹¶å®‰è£…[æ”¯æŒ NVIDIA CUDA çš„ WSL é©±åŠ¨ç¨‹åº](https://www.nvidia.com/download/index.aspx)ï¼Œä»¥ç”¨äºç°æœ‰çš„ CUDA ML å·¥ä½œæµã€‚

## å®‰è£… WSL2

å‚è€ƒå†å²æ–‡ç«  [Windows å®‰è£… WSL2](/posts/a8debf73)

### å®‰è£… CUDA Toolkit

ä»è¿™ä¸€ç‚¹å¼€å§‹ï¼Œæ‚¨åº”è¯¥èƒ½å¤Ÿè¿è¡Œä»»ä½•éœ€è¦ CUDA çš„ç°æœ‰ Linux åº”ç”¨ç¨‹åºã€‚**ä¸è¦åœ¨ WSL ç¯å¢ƒä¸­å®‰è£…ä»»ä½•é©±åŠ¨ç¨‹åº**ã€‚è¦æ„å»º CUDA åº”ç”¨ç¨‹åºï¼Œæ‚¨å°†éœ€è¦ CUDA å·¥å…·åŒ…ã€‚

åœ¨ç³»ç»Ÿä¸Šå®‰è£… Windows NVIDIA GPU é©±åŠ¨ç¨‹åºåï¼ŒCUDA å°†åœ¨ WSL 2 ä¸­å¯ç”¨ã€‚å®‰è£…åœ¨ Windows ä¸»æœºä¸Šçš„ CUDA é©±åŠ¨ç¨‹åºå°†åœ¨ WSL 2 ä¸­å­˜æ ¹ï¼Œå› æ­¤**ç”¨æˆ·ä¸å¾—åœ¨ WSL 2 ä¸­å®‰è£…ä»»ä½• NVIDIA GPU Linux é©±åŠ¨ç¨‹åº**ã€‚è¿™é‡Œå¿…é¡»éå¸¸å°å¿ƒï¼Œå› ä¸ºé»˜è®¤çš„ CUDA å·¥å…·åŒ…é™„å¸¦é©±åŠ¨ç¨‹åºï¼Œå¹¶ä¸”å¾ˆå®¹æ˜“ä½¿ç”¨é»˜è®¤å®‰è£…è¦†ç›– WSL 2 NVIDIA é©±åŠ¨ç¨‹åºã€‚å»ºè®®å¼€å‘äººå‘˜ä½¿ç”¨[å•ç‹¬çš„ CUDA Toolkit](https://developer.nvidia.com/cuda-downloads?target_os=Linux\&target_arch=x86_64\&Distribution=WSL-Ubuntu\&target_version=2.0) for WSL 2 ï¼ˆUbuntuï¼‰ ä¸‹è½½é¡µé¢ä¸­æä¾›çš„ CUDA Toolkitï¼Œä»¥é¿å…æ­¤è¦†ç›–ã€‚

é¦–å…ˆï¼Œåˆ é™¤æ—§çš„ GPG å¯†é’¥ï¼š

```sh
sudo apt-key del 7fa2af80
sudo apt install gcc dkms -y
## æ–¹å¼ä¸€ Installer Type runfile
wget https://developer.download.nvidia.com/compute/cuda/12.5.1/local_installers/cuda_12.5.1_555.42.06_linux.run
sudo sh cuda_12.5.1_555.42.06_linux.run


## æ–¹å¼äºŒ debæ–¹å¼
sudo cp cuda-wsl-ubuntu.pin /etc/apt/preferences.d/cuda-repository-pin-600
sudo dpkg -i cuda-repo-wsl-ubuntu-12-5-local_12.5.1-1_amd64.deb
sudo cp /var/cuda-repo-wsl-ubuntu-12-5-local/cuda-*-keyring.gpg /usr/share/keyrings/
sudo apt-get update
sudo apt-get -y install cuda-toolkit-12-5
```

## WSL2 å®‰è£… Docker

å‚è€ƒå†å²æ–‡ç«  [WSL2 å®‰è£… Docker](/posts/1882513b)

## å®‰è£… NVIDIA å®¹å™¨å·¥å…·åŒ…

1. é…ç½®ç”Ÿäº§å­˜å‚¨åº“ï¼š

   ```sh
   curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg \
     && curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list | \
       sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | \
       sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list
   ```

2. ä»å­˜å‚¨åº“æ›´æ–°è½¯ä»¶åŒ…åˆ—è¡¨ï¼š

   ```
   sudo apt-get update
   ```

3. å®‰è£… NVIDIA Container Toolkit è½¯ä»¶åŒ…ï¼š

   ```
   sudo apt-get install -y nvidia-container-toolkit
   ```

4. ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤é…ç½®å®¹å™¨è¿è¡Œæ—¶ï¼š`nvidia-ctk`

   ```shell
   # è¯¥å‘½ä»¤ä¿®æ”¹ä¸»æœºä¸Šçš„/etc/docker/daemon.jsonï¼Œä»¥ä¾¿ Docker å¯ä»¥ä½¿ç”¨ NVIDIA å®¹å™¨è¿è¡Œæ—¶ã€‚
   sudo nvidia-ctk runtime configure --runtime=docker
   # é‡æ–°å¯åŠ¨ Docker å®ˆæŠ¤ç¨‹åºï¼š
   sudo systemctl restart docker
   ```

5. æµ‹è¯•

   ```shell
   sudo docker run --rm --runtime=nvidia --gpus all ubuntu nvidia-smi
   sudo docker run --rm  --gpus=all ubuntu nvidia-smi -L
   sudo docker run -it --gpus=all --rm nvidia/cuda:12.2.0-base-ubuntu20.04 nvidia-smi
   sudo docker run --gpus=all --rm nvcr.io/nvidia/k8s/cuda-sample:nbody nbody -gpu -benchmark
   sudo docker run --rm --runtime nvidia --gpus all pytorch/pytorch:2.0.0-cuda11.7-cudnn8-runtime nvidia-smi
   ```

### docker-desktop

> å¦‚æœä½¿ç”¨çš„æ˜¯ WSL+dockerdesktop

![](/resources/9926b8a199d847d29375920077c13521.png)

```json
{
  "builder": {
    "gc": {
      "defaultKeepStorage": "20GB",
      "enabled": true
    }
  },
  "experimental": false,
  "runtimes": {
    "nvidia": {
      "path": "/usr/bin/nvidia-container-runtime",
      "runtimeArgs": []
    }
  }
}
```

### é—®é¢˜ 1

```sh
root@DESKTOP-8DE2K5S:/mnt/c/Users/fulsun# sudo docker run --rm --runtime=nvidia --gpus all ubuntu nvidia-smi
Failed to initialize NVML: GPU access blocked by the operating system
Failed to properly shut down NVML: GPU access blocked by the operating system
```

> è§£å†³æ–¹æ¡ˆï¼š [åœ¨ WSL2 çš„ docker å®¹å™¨ä¸­ï¼šæ“ä½œç³»ç»Ÿé˜»æ­¢çš„ GPU è®¿é—® Â·é—®é¢˜ #9962 Â·å¾®è½¯/WSL Â·GitHub ä¸Š](https://github.com/microsoft/WSL/issues/9962)

```sh
åœ¨æ–‡ä»¶ /etc/nvidia-container-runtime/config.toml ä¸­ï¼Œå°†  no-cgroups  ä»  true  æ›´æ”¹ä¸º false
```

### é—®é¢˜ 2

```sh
root@DESKTOP-8DE2K5S:/mnt/g/å¤§æ¨¡å‹/nvida/cuda# nvidia-smi
Sat Jul 20 01:37:58 2024
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 555.58.02              Driver Version: 556.12         CUDA Version: 12.5     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
Segmentation fault
```

> é”™è¯¯é—®é¢˜ [nvidia-smi segmentation fault in wsl2 but not in Windows Â· Issue #11277 Â· microsoft/WSL Â· GitHub](https://github.com/microsoft/WSL/issues/11277)

ä¸‹è½½å®‰è£… 537 ç‰ˆæœ¬çš„é©±åŠ¨ï¼š<https://developer.nvidia.com/downloads/vulkan-beta-53796-windows>

## Ollama å®‰è£…

<https://ollama.com/>
<https://github.com/ollama/ollama/blob/main/docs/linux.md>
<https://github.com/ollama/ollama/blob/main/docs/faq.md>

### Docker æ–¹å¼å®‰è£…

```shell
docker pull ollama/ollama
# é»˜è®¤å­˜æ”¾åœ¨å®¿ä¸»æœºçš„ /var/lib/docker/volumes/æ•°æ®å·åç§°/_data
docker volume create ollama
# CPU only
docker run -d -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama
# Nvidia GPU
docker run -d --gpus=all -v ollama:/root/.ollama -p 11434:11434 --env OLLAMA_KEEP_ALIVE=60m --name ollama ollama/ollama
# AMD GPU
docker run -d --device /dev/kfd --device /dev/dri -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama:rocm
# æœ¬åœ°è¿è¡Œæ¨¡å‹
docker exec -it ollama ollama run gemma:2b
docker exec -it ollama /bin/bash
# è®¾ç½®è‡ªåŠ¨å¯åŠ¨
docker update --restart=always å®¹å™¨ID(æˆ–è€…å®¹å™¨å)


## ç¯å¢ƒå˜é‡æŸ¥çœ‹
docker run -d --gpus=all -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama
2f2104f18c08f37b4e79e3a5aa35f2b9f4240659f6a1786e6d4b504aeb5b2e03
docker exec 2f2104f1 env
PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
HOSTNAME=2f2104f18c08
OLLAMA_HOST=0.0.0.0
LD_LIBRARY_PATH=/usr/local/nvidia/lib:/usr/local/nvidia/lib64
NVIDIA_DRIVER_CAPABILITIES=compute,utility
NVIDIA_VISIBLE_DEVICES=all
HOME=/root


# å¯¼å‡ºå¯¼å…¥
# docker saveä¿å­˜çš„æ˜¯é•œåƒï¼ˆimageï¼‰ï¼Œdocker exportä¿å­˜çš„æ˜¯å®¹å™¨ï¼ˆcontainerï¼‰ï¼›
# docker loadç”¨æ¥è½½å…¥é•œåƒåŒ…ï¼Œdocker importç”¨æ¥è½½å…¥å®¹å™¨åŒ…ï¼Œä½†ä¸¤è€…éƒ½ä¼šæ¢å¤ä¸ºé•œåƒï¼›
# docker loadä¸èƒ½å¯¹è½½å…¥çš„é•œåƒé‡å‘½åï¼Œè€Œdocker importå¯ä»¥ä¸ºé•œåƒæŒ‡å®šæ–°åç§°ã€‚
cd /mnt/h/å¤§æ¨¡å‹/docker/
docker save -o ollama.tar ollama/ollama
docker load -i ollama.tar
```

### åœ¨ Linux ä¸Šè®¾ç½®ç¯å¢ƒå˜é‡(é docker)

- OLLAMA\_HOSTï¼šè¿™ä¸ªå˜é‡å®šä¹‰äº† Ollama ç›‘å¬çš„ç½‘ç»œæ¥å£ã€‚é€šè¿‡è®¾ç½® OLLAMA\_HOST=0.0.0.0ï¼Œæˆ‘ä»¬å¯ä»¥è®© Ollama ç›‘å¬æ‰€æœ‰å¯ç”¨çš„ç½‘ç»œæ¥å£ï¼Œä»è€Œå…è®¸å¤–éƒ¨ç½‘ç»œè®¿é—®ã€‚
- OLLAMA\_MODELSï¼šè¿™ä¸ªå˜é‡æŒ‡å®šäº†æ¨¡å‹é•œåƒçš„å­˜å‚¨è·¯å¾„ã€‚é€šè¿‡è®¾ç½® OLLAMA\_MODELS=E:\OllamaCacheï¼Œæˆ‘ä»¬å¯ä»¥å°†æ¨¡å‹é•œåƒå­˜å‚¨åœ¨ E ç›˜ï¼Œé¿å… C ç›˜ç©ºé—´ä¸è¶³çš„é—®é¢˜ã€‚
- OLLAMA\_KEEP\_ALIVEï¼šè¿™ä¸ªå˜é‡æ§åˆ¶æ¨¡å‹åœ¨å†…å­˜ä¸­çš„å­˜æ´»æ—¶é—´ã€‚è®¾ç½® OLLAMA\_KEEP\_ALIVE=24h å¯ä»¥è®©æ¨¡å‹åœ¨å†…å­˜ä¸­ä¿æŒ 24 å°æ—¶ï¼Œæé«˜è®¿é—®é€Ÿåº¦ã€‚
- OLLAMA\_PORTï¼šè¿™ä¸ªå˜é‡å…è®¸æˆ‘ä»¬æ›´æ”¹ Ollama çš„é»˜è®¤ç«¯å£ã€‚ä¾‹å¦‚ï¼Œè®¾ç½® OLLAMA\_PORT=8080 å¯ä»¥å°†æœåŠ¡ç«¯å£ä»é»˜è®¤çš„ 11434 æ›´æ”¹ä¸º 8080ã€‚
- OLLAMA\_NUM\_PARALLELï¼šè¿™ä¸ªå˜é‡å†³å®šäº† Ollama å¯ä»¥åŒæ—¶å¤„ç†çš„ç”¨æˆ·è¯·æ±‚æ•°é‡ã€‚è®¾ç½® OLLAMA\_NUM\_PARALLEL=4 å¯ä»¥è®© Ollama åŒæ—¶å¤„ç†ä¸¤ä¸ªå¹¶å‘è¯·æ±‚ã€‚
- OLLAMA\_MAX\_LOADED\_MODELSï¼šè¿™ä¸ªå˜é‡é™åˆ¶äº† Ollama å¯ä»¥åŒæ—¶åŠ è½½çš„æ¨¡å‹æ•°é‡ã€‚è®¾ç½® OLLAMA\_MAX\_LOADED\_MODELS=4 å¯ä»¥ç¡®ä¿ç³»ç»Ÿèµ„æºå¾—åˆ°åˆç†åˆ†é…ã€‚

```sh
sudo vi /etc/systemd/system/ollama.service
# æ·»åŠ ä»¥ä¸‹å†…å®¹
Environment="OLLAMA_HOST=0.0.0.0:11434"
Environment="OLLAMA_KEEP_ALIVE=60m"
Environment="OLLAMA_MAX_LOADED_MODELS=1"
ExecStart=/usr/local/bin/ollama serve

# é‡è½½ systemd å¹¶é‡å¯ Ollamaï¼š
systemctl daemon-reload
sudo systemctl start ollama

# æ£€æŸ¥
curl http://localhost:11434
æ˜¾ç¤º Ollama is running

```

```sh
# ä¸‹è½½æ¨¡å‹
ollama pull gemma:2b
ollama list
ollama run gemma:2b
```

## å¯¼å…¥æ¨¡å‹

### å¯¼å…¥ (GGUF)

1. é¦–å…ˆåˆ›å»ºä¸€ä¸ª Modelfileã€‚ æ­¤æ–‡ä»¶æ˜¯æ¨¡å‹çš„è“å›¾ï¼Œç”¨äºæŒ‡å®šæƒé‡ã€å‚æ•°ã€æç¤ºæ¨¡æ¿ç­‰ã€‚

   ```shell
   cp /mnt/h/å¤§æ¨¡å‹/æ¨¡å‹/gguf/qwen2-7b-instruct-q4_k_s.gguf /var/lib/docker/volumes/ollama/_data/
   docker exec -it ollama /bin/bash
   cd /root/.ollama/
   vi Modelfile
   FROM ./qwen2-7b-instruct-q4_k_s.gguf
   # ï¼ˆå¯é€‰ï¼‰è®¸å¤šèŠå¤©æ¨¡å‹éœ€è¦æç¤ºæ¨¡æ¿æ‰èƒ½æ­£ç¡®å›ç­”
   FROM ./qwen2-7b-instruct-q4_k_s.gguf
   TEMPLATE "[INST] {{ .Prompt }} [/INST]"
   ```

2. åˆ›å»º Ollama æ¨¡å‹

   ```shell
   ollama create qwen2-7b-instruct-q4_k_s.gguf -f Modelfile
   transferring model data â ‹
   ```

3. ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤æµ‹è¯•æ¨¡å‹ï¼š`ollama run`

   ```shell
   ollama run qwen2-7b-instruct-q4_k_s.gguf "What is your favourite condiment?"
   ```

### å¯¼å…¥ï¼ˆPyTorch å’Œ Safetensorsï¼‰

> ä» PyTorch å’Œ Safetensors å¯¼å…¥çš„è¿‡ç¨‹æ¯”ä» GGUF å¯¼å…¥è¦é•¿ã€‚ä½¿å®ƒæ›´å®¹æ˜“çš„æ”¹è¿›æ­£åœ¨è¿›è¡Œä¸­ã€‚

1. é¦–å…ˆï¼Œå…‹éš†å­˜å‚¨åº“ï¼š`ollama/ollama`

   ```sh
   git clone git@github.com:ollama/ollama.git ollama
   cd ollama
   ```

2. ç„¶åè·å–å…¶å­æ¨¡å—ï¼š`llama.cpp`

   ```
   git submodule init
   git submodule update llm/llama.cpp
   ```

3. æ¥ä¸‹æ¥ï¼Œå®‰è£… Python ä¾èµ–é¡¹ï¼š

   ```
   python3 -m venv llm/llama.cpp/.venv
   source llm/llama.cpp/.venv/bin/activate
   pip install -r llm/llama.cpp/requirements.txt
   ```

4. ç„¶åæ„å»ºå·¥å…·ï¼š`quantize`

   ```
   make -C llm/llama.cpp quantize
   ```

5. å¦‚æœæ¨¡å‹å½“å‰æ‰˜ç®¡åœ¨ HuggingFace å­˜å‚¨åº“ä¸­ï¼Œè¯·é¦–å…ˆå…‹éš†è¯¥å­˜å‚¨åº“ä»¥ä¸‹è½½åŸå§‹æ¨¡å‹ã€‚ï¼ˆå¯é€‰ï¼‰

   ```shell
   # å®‰è£…Git LFSï¼ŒéªŒè¯å®ƒæ˜¯å¦å·²å®‰è£…ï¼Œç„¶åå…‹éš†æ¨¡å‹çš„å­˜å‚¨åº“ï¼š
   git lfs install
   git clone https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1 model
   ```

6. **è½¬æ¢æ¨¡å‹** æ³¨æ„ï¼šæŸäº›æ¨¡å‹æ¶æ„éœ€è¦ä½¿ç”¨ç‰¹å®šçš„è½¬æ¢è„šæœ¬ã€‚ä¾‹å¦‚ï¼ŒQwen æ¨¡å‹éœ€è¦è¿è¡Œ`convert-hf-to-gguf.py`è€Œä¸æ˜¯`convert.py`

   ```shell
   python llm/llama.cpp/convert.py ./model --outtype f16 --outfile converted.bin
   ```

7. é‡åŒ–æ¨¡å‹

   ```sh
   llm/llama.cpp/quantize converted.bin quantized.bin q4_0
   ```

8. ä¸ºæ‚¨çš„æ¨¡å‹åˆ›å»ºä¸€ä¸ªï¼š`Modelfile`

   ```shell
   FROM quantized.bin
   TEMPLATE "[INST] {{ .Prompt }} [/INST]"
   ```

9. åˆ›å»º Ollama æ¨¡å‹

   ```shell
   ollama create example -f Modelfile
   ```

10. è¿è¡Œæ¨¡å‹

    ```shell
    ollama run example "What is your favourite condiment?"
    ```

## è°ƒç”¨æ¨¡å‹

### Curl è°ƒç”¨æ¨¡å‹

```sh
curl http://localhost:11434/api/chat -d '{"model": "gemma:2b","messages": [{ "role": "user", "content": "who are you" }]}'
```

### Python è°ƒç”¨æ¨¡å‹

ï¼ˆPython ä»£ç è°ƒç”¨ï¼Œå»ºè®®åœ¨ conda ä¸‹è¿è¡Œ)

```python
vim test.py

import requests
import json

def send_message_to_ollama(message, port=11434):
url = f"http://localhost:{port}/api/chat"
payload = {
"model": "gemma:2b",
"messages": [{"role": "user", "content": message}]
}
response = requests.post(url, json=payload)
if response.status_code == 200:
response_content = ""
for line in response.iter_lines():
if line:
response_content += json.loads(line)["message"]["content"]
return response_content
else:
return f"Error: {response.status_code} - {response.text}"

if __name__ == "__main__":
user_input = "who are you"
response = send_message_to_ollama(user_input)
print("Ollama's response:")
print(response)
```

åœ¨ conda ç¯å¢ƒä¸­è¿è¡Œ Python è„šæœ¬

```
conda activate XXX
pip install requests
python test.py
conda deactivate
```

## Open WebUI éƒ¨ç½²

<https://openwebui.com/>
<https://github.com/open-webui/open-webui>

### Docker æ–¹å¼å®‰è£…

```sh
docker images
docker pull ghcr.io/open-webui/open-webui:main

cd /mnt/h/å¤§æ¨¡å‹/docker/
docker save -o open-webui.tar ghcr.io/open-webui/open-webui:main
docker load -i open-webui.tar
```

```shell
# Ollama åœ¨åŒä¸€å°ç”µè„‘
docker run -d -p 3000:8080 --add-host=host.docker.internal:host-gateway -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main
# hostæ¨¡å¼
docker run -d --network=host -v open-webui:/app/backend/data -e OLLAMA_BASE_URL=http://127.0.0.1:11434 --name open-webui88 --restart always ghcr.io/open-webui/open-webui:main

# Ollama åœ¨ä¸åŒä¸€å°ç”µè„‘
docker run -d -p 3000:8080 -e OLLAMA_BASE_URL=https://example.com -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main

# æ›´æ–° open-webui
docker run --rm --volume /var/run/docker.sock:/var/run/docker.sock containrrr/watchtower --run-once open-webui
```

éƒ¨ç½²åå¯åŠ¨ï¼ˆæ—¶é—´æœ‰ç‚¹é•¿ï¼‰ æ‰“å¼€ç½‘ç«™ <http://127.0.0.1:3000å…ˆæ³¨å†Œï¼Œé€‰æ‹©æ¨¡å¼ï¼ˆæ­£å¸¸æƒ…å†µï¼Œä¼šæ˜¾ç¤ºå‡ºåˆšæ‰ollamaå·²ç»ä¸‹è½½å¥½çš„æ¨¡å‹ï¼‰>

### Installation with `pip` (Beta)[](https://docs.openwebui.com/getting-started/#installation-with-pip-beta)

For users who prefer to use Python's package manager `pip`, Open WebUI offers a installation method. Python 3.11 is required for this method.

1. **Install Open WebUI**: Open your terminal and run the following command:

   ```bash
   pip install open-webui
   ```

2. **Start Open WebUI**: Once installed, start the server using:

   ```bash
   open-webui serve
   ```

This method installs all necessary dependencies and starts Open WebUI, allowing for a simple and efficient setup. After installation, you can access Open WebUI at [http://localhost:8080](http://localhost:8080/). Enjoy! ğŸ˜„

### Install from Open WebUI Github Repo[](https://docs.openwebui.com/getting-started/#install-from-open-webui-github-repo)

INFO

Open WebUI consists of two primary components: the frontend and the backend (which serves as a reverse proxy, handling static frontend files, and additional features). Both need to be running concurrently for the development environment.

#### Requirements ğŸ“¦[](https://docs.openwebui.com/getting-started/#requirements-)

- ğŸ° [Node.js](https://nodejs.org/en) >= 20.10
- ğŸ [Python](https://python.org/) >= 3.11

#### Build and Install ğŸ› ï¸[](https://docs.openwebui.com/getting-started/#build-and-install-ï¸)

Run the following commands to install:

```sh
git clone https://github.com/open-webui/open-webui.git
cd open-webui/

# Copying required .env file
cp -RPp .env.example .env

# Building Frontend Using Node
npm i
npm run build

# Serving Frontend with the Backend
cd ./backend
pip install -r requirements.txt -U
bash start.sh
```

You should have Open WebUI up and running at <http://localhost:8080/>. Enjoy! ğŸ˜„

## ollama ç¯å¢ƒå‚æ•°

1. `OLLAMA_HOST`ï¼šè¿™ä¸ªå˜é‡å®šä¹‰äº† Ollama ç›‘å¬çš„[ç½‘ç»œæ¥å£](https://so.csdn.net/so/search?q=ç½‘ç»œæ¥å£\&spm=1001.2101.3001.7020)ã€‚é€šè¿‡è®¾ç½® OLLAMA\_HOST=0.0.0.0ï¼Œæˆ‘ä»¬å¯ä»¥è®© Ollama ç›‘å¬æ‰€æœ‰å¯ç”¨çš„ç½‘ç»œæ¥å£ï¼Œä»è€Œå…è®¸å¤–éƒ¨ç½‘ç»œè®¿é—®ã€‚
2. `OLLAMA_MODELS`ï¼šè¿™ä¸ªå˜é‡æŒ‡å®šäº†æ¨¡å‹é•œåƒçš„å­˜å‚¨è·¯å¾„ã€‚é€šè¿‡è®¾ç½® OLLAMA\_MODELS=F:\OllamaCacheï¼Œæˆ‘ä»¬å¯ä»¥å°†æ¨¡å‹é•œåƒå­˜å‚¨åœ¨ E ç›˜ï¼Œé¿å… C ç›˜ç©ºé—´ä¸è¶³çš„é—®é¢˜ã€‚
3. `OLLAMA_KEEP_ALIVE`ï¼šè¿™ä¸ªå˜é‡æ§åˆ¶æ¨¡å‹åœ¨å†…å­˜ä¸­çš„å­˜æ´»æ—¶é—´ã€‚è®¾ç½® OLLAMA\_KEEP\_ALIVE=24h å¯ä»¥è®©æ¨¡å‹åœ¨å†…å­˜ä¸­ä¿æŒ 24 å°æ—¶ï¼Œæé«˜è®¿é—®é€Ÿåº¦ã€‚
4. `OLLAMA_PORT`ï¼šè¿™ä¸ªå˜é‡å…è®¸æˆ‘ä»¬æ›´æ”¹ Ollama çš„é»˜è®¤ç«¯å£ã€‚ä¾‹å¦‚ï¼Œè®¾ç½® OLLAMA\_PORT=8080 å¯ä»¥å°†æœåŠ¡ç«¯å£ä»é»˜è®¤çš„ 11434 æ›´æ”¹ä¸º 8080ã€‚
5. `OLLAMA_NUM_PARALLEL`ï¼šè¿™ä¸ªå˜é‡å†³å®šäº† Ollama å¯ä»¥åŒæ—¶å¤„ç†çš„ç”¨æˆ·è¯·æ±‚æ•°é‡ã€‚è®¾ç½® OLLAMA\_NUM\_PARALLEL=4 å¯ä»¥è®© Ollama åŒæ—¶å¤„ç†ä¸¤ä¸ªå¹¶å‘è¯·æ±‚ã€‚
6. `OLLAMA_MAX_LOADED_MODELS`ï¼šè¿™ä¸ªå˜é‡é™åˆ¶äº† Ollama å¯ä»¥åŒæ—¶åŠ è½½çš„æ¨¡å‹æ•°é‡ã€‚è®¾ç½® OLLAMA\_MAX\_LOADED\_MODELS=4 å¯ä»¥ç¡®ä¿[ç³»ç»Ÿèµ„æº](https://so.csdn.net/so/search?q=ç³»ç»Ÿèµ„æº\&spm=1001.2101.3001.7020)å¾—åˆ°åˆç†åˆ†é…ã€‚
